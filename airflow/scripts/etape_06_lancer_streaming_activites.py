# ======================================================================================
# Script      : etape_06_lancer_streaming_activites.py
# Objectif    : Lancer manuellement le job Spark streaming Kafka ‚Üí Delta Lake
#               - Ce script d√©clenche un traitement en continu des activit√©s sportives
#               - Lecture depuis Kafka (Debezium), √©criture dans Delta Lake (MinIO)
#               - Utilisable √† la main ou via un DAG Airflow
# Auteur      : Xavier Rousseau | Juillet 2025
# ======================================================================================

import subprocess
import os
from dotenv import load_dotenv
from loguru import logger

# ======================================================================================
# 1. Chargement des variables d‚Äôenvironnement (.env global mont√© dans Airflow)
# ======================================================================================

load_dotenv(dotenv_path="/opt/airflow/.env", override=True)

SPARK_CONTAINER_NAME = os.getenv("SPARK_CONTAINER_NAME", "sport-spark")  # Nom du conteneur Spark
SPARK_STREAMING_SCRIPT = os.getenv("SPARK_SCRIPT_STREAMING_PATH", "/opt/airflow/jobs/bronze_streaming_activites.py")

# ======================================================================================
# 2. Fonction principale appel√©e par Airflow ou en local
# ======================================================================================

def main(**kwargs):
    """
    Lancement manuel du job Spark Streaming (Kafka ‚Üí Delta Lake)
    ------------------------------------------------------------
    Ce job lit les messages Kafka contenant les activit√©s sportives
    (produits par Debezium/PostgreSQL), puis :
      - parse les messages JSON
      - valide les champs attendus
      - √©crit le flux dans Delta Lake (bronze) sur MinIO

    La commande est lanc√©e en arri√®re-plan avec subprocess.Popen()
    pour ne pas bloquer Airflow.
    """
    logger.info("üöÄ Lancement du job Spark Streaming (Kafka ‚Üí Delta Lake)")

    cmd = [
        "docker", "exec", "-i", SPARK_CONTAINER_NAME,
        "spark-submit", SPARK_STREAMING_SCRIPT
    ]

    try:
        logger.debug(f"üõ†Ô∏è Commande ex√©cut√©e : {' '.join(cmd)}")
        subprocess.Popen(cmd)
        logger.success("‚úÖ Job Spark Streaming lanc√© en arri√®re-plan.")
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du lancement Spark Streaming : {e}")
        raise

# ======================================================================================
# 3. Point d‚Äôentr√©e direct (utilisation en CLI si besoin)
# ======================================================================================

if __name__ == "__main__":
    main()
