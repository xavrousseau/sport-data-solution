# ======================================================================================
# DAG       : dag_test_streaming_kafka_delta.py
# Objectif  : Tester le flux Kafka â†’ Spark â†’ Delta Lake indÃ©pendamment
# Auteur    : Xavier Rousseau | Juillet 2025
# ======================================================================================

from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.operators.dummy_operator import DummyOperator
from datetime import datetime
import subprocess
from loguru import logger

# ======================================================================================
# 1. DÃ©finition de la tÃ¢che : lancement du job Spark Streaming
# ======================================================================================

def lancer_streaming_spark():
    """
    Lance le job Spark de streaming Kafka â†’ Delta.
    Ce test permet de vÃ©rifier que les donnÃ©es Kafka sont bien Ã©crites dans Delta Lake.
    """
    cmd = [
        "docker", "exec", "-i", "sport-spark",
        "spark-submit", "/opt/bitnami/spark/jobs/bronze_streaming_activites.py"
    ]
    try:
        logger.info(f"ğŸš€ Lancement job Spark streaming : {' '.join(cmd)}")
        subprocess.Popen(cmd)  # Non bloquant
        logger.success("âœ… Job Spark Streaming lancÃ© en arriÃ¨re-plan")
    except Exception as e:
        logger.error(f"âŒ Erreur lancement Spark : {e}")
        raise

# ======================================================================================
# 2. ParamÃ¨tres du DAG
# ======================================================================================

default_args = {
    "owner": "airflow",
    "start_date": datetime(2025, 7, 1),
    "retries": 0,
}

# ======================================================================================
# 3. DÃ©finition du DAG minimal de test
# ======================================================================================

with DAG(
    dag_id="test_kafka_spark_delta",
    description="Test de la chaÃ®ne Kafka â†’ Spark Streaming â†’ Delta Lake",
    default_args=default_args,
    schedule_interval=None,
    catchup=False,
    tags=["test", "streaming", "kafka", "delta"],
) as dag:

    start_test = DummyOperator(task_id="start_test")

    lancer_spark = PythonOperator(
        task_id="lancer_spark_streaming",
        python_callable=lancer_streaming_spark,
    )

    end_test = DummyOperator(task_id="end_test")

    # Orchestration simple
    start_test >> lancer_spark >> end_test
