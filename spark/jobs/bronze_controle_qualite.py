# ======================================================================================
# Script      : bronze_controle_qualite.py
# Objectif    : Contr√¥le qualit√© des activit√©s sportives (bronze / Delta Lake)
#               - V√©rifie plusieurs r√®gles de coh√©rence m√©tier
#               - Exporte les erreurs dans MinIO (Excel)
#               - Envoie un message de notification ntfy
# Auteur      : Xavier Rousseau | Juillet 2025
# ======================================================================================

import os
import sys
import requests
import pandas as pd
from dotenv import load_dotenv
from loguru import logger
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from minio import Minio
from delta import configure_spark_with_delta_pip

# ======================================================================================
# 1. Chargement configuration depuis le fichier .env
# ======================================================================================

load_dotenv(dotenv_path=".env")

MINIO_HOST = os.getenv("MINIO_HOST", "sport-minio")
MINIO_ENDPOINT = f"{MINIO_HOST}:9000"
MINIO_ACCESS_KEY = os.getenv("MINIO_ROOT_USER", "minio")
MINIO_SECRET_KEY = os.getenv("MINIO_ROOT_PASSWORD", "minio123")
DELTA_PATH = os.getenv("DELTA_PATH_ACTIVITES", "s3a://sportdata/resultats/")
NTFY_TOPIC = os.getenv("NTFY_TOPIC", "sportdata_activites")
NTFY_URL = os.getenv("NTFY_URL", f"http://localhost:8080/{NTFY_TOPIC}")

EXPORT_KEY = "exports/erreurs_qualite_activites.xlsx"
EXPORT_PATH = "/tmp/erreurs_qualite_activites.xlsx"

# ======================================================================================
# 2. Fonction principale du contr√¥le qualit√©
# ======================================================================================

def controle_qualite():
    logger.info("üöÄ Initialisation de SparkSession avec support Delta + S3A")

    builder = SparkSession.builder \
        .appName("Contr√¥le Qualit√© Activit√©s Sportives") \
        .master("local[*]") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.hadoop.fs.AbstractFileSystem.s3a.impl", "org.apache.hadoop.fs.s3a.S3A") \
        .config("spark.hadoop.fs.s3a.endpoint", f"http://{MINIO_ENDPOINT}") \
        .config("spark.hadoop.fs.s3a.access.key", MINIO_ACCESS_KEY) \
        .config("spark.hadoop.fs.s3a.secret.key", MINIO_SECRET_KEY) \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")

    try:
        spark = configure_spark_with_delta_pip(builder).getOrCreate()
        logger.info("‚úÖ SparkSession instanci√©e avec succ√®s")
    except Exception as e:
        logger.error(f"‚ùå √âchec d‚Äôinstanciation Spark : {e}")
        raise

    spark.sparkContext.setLogLevel("WARN")

    # Lecture des donn√©es Delta depuis MinIO
    try:
        logger.info(f"üìÇ Lecture des donn√©es Delta : {DELTA_PATH}")
        df = spark.read.format("delta").load(DELTA_PATH)
        logger.success(f"‚úÖ Donn√©es charg√©es depuis : {DELTA_PATH}")
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la lecture Delta : {e}")
        spark.stop()
        raise

    # R√®gles m√©tier √† appliquer
    regles = [
        ("distance_km > 0", col("distance_km") > 0),
        ("temps_sec > 0", col("temps_sec") > 0),
        ("type_activite NOT NULL", col("type_activite").isNotNull()),
        ("id_salarie NOT NULL", col("id_salarie").isNotNull())
    ]

    df_valide = df
    df_erreurs = spark.createDataFrame([], df.schema)

    for nom_regle, condition in regles:
        violations = df_valide.filter(~condition)
        nb = violations.count()
        if nb > 0:
            logger.warning(f"‚ùå {nb} violation(s) : {nom_regle}")
            df_erreurs = df_erreurs.union(violations)
            df_valide = df_valide.filter(condition)
        else:
            logger.info(f"‚úÖ R√®gle OK : {nom_regle}")

    nb_valides = df_valide.count()
    nb_erreurs = df_erreurs.count()

    logger.info("üìä R√©sum√© du contr√¥le qualit√© :")
    logger.info(f"‚úîÔ∏è Lignes valides   : {nb_valides}")
    logger.info(f"‚ùå Lignes en erreur : {nb_erreurs}")

    # Export des erreurs en Excel vers MinIO
    if nb_erreurs > 0:
        try:
            logger.info("üì§ Export des erreurs au format Excel...")
            df_erreurs.toPandas().to_excel(EXPORT_PATH, index=False)

            minio = Minio(
                MINIO_HOST,
                access_key=MINIO_ACCESS_KEY,
                secret_key=MINIO_SECRET_KEY,
                secure=False
            )
            minio.fput_object("datalake", EXPORT_KEY, EXPORT_PATH)
            logger.success(f"‚úÖ Erreurs export√©es dans MinIO : {EXPORT_KEY}")
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l‚Äôexport Excel vers MinIO : {e}")

    # Notification via ntfy
    try:
        message = (
            f"‚ùå Contr√¥le qualit√© : {nb_erreurs} erreur(s) d√©tect√©e(s)."
            if nb_erreurs > 0
            else f"‚úÖ Contr√¥le qualit√© OK : {nb_valides} lignes valides."
        )
        requests.post(NTFY_URL, data=message.encode("utf-8"))
        logger.info(f"üîî Notification ntfy envoy√©e : {message}")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è √âchec d‚Äôenvoi de notification ntfy : {e}")

    if nb_erreurs > 0:
        logger.info("üîç Exemples d‚Äôerreurs d√©tect√©es :")
        df_erreurs.show(10, truncate=False)

    spark.stop()

# ======================================================================================
# 3. Lancement direct (CLI ou Airflow)
# ======================================================================================

def main():
    controle_qualite()

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logger.error(f"‚ùå Erreur inattendue : {e}")
        sys.exit(1)
