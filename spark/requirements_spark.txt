# ============================================================================
# Fichier    : spark/requirements.txt
# Objectif   : Dépendances Python pour les scripts Spark de la stack Sport Data Solution
# Auteur     : Xavier Rousseau | Août 2025 (version complète et corrigée)
# ============================================================================

########## NOTIFICATIONS / API HTTP ##########
requests==2.32.2                  # Appels HTTP vers ntfy, Slack, etc.
python-dotenv==1.0.1              # Chargement des variables d’environnement .env

########## LOGGING ##########
loguru==0.7.2                     # Logging lisible et structuré dans les jobs Spark

########## MANIPULATION DE DONNÉES ##########
pandas==2.2.2                     # Manipulation de données tabulaires (toPandas, export Excel)
openpyxl==3.1.2                   # Export Excel depuis pandas

########## MINIO ##########
minio==7.2.5                      # Client MinIO compatible avec S3 (upload vers MinIO)

########## SUPPORT DELTA LAKE POUR PYSPARK ##########
delta-spark==3.1.0                # Support Delta Lake avec Spark 3.5.x
# Référence : https://pypi.org/project/delta-spark/

########## SPARK STREAMING (optionnel) ##########
confluent-kafka==2.3.0            # Client Kafka (si besoin en simulation ou test)

########## SIMULATION / TEST ##########
faker==24.8.0                     # Génération de données fictives

########## TEMPS / DATES ##########
pendulum==3.0.0                   # Manipulation de dates robuste, compatible avec Spark

########## EXPORTS SQL / POSTGRESQL ##########
sqlalchemy==2.0.30                # Export tables PostgreSQL via pandas
psycopg2-binary==2.9.9            # Driver PostgreSQL natif pour SQLAlchemy
# (psycopg2 classique fonctionne aussi, mais -binary est plus simple pour l’intégration)

########## CONTRÔLE QUALITÉ ##########
great_expectations==0.18.14       # Data quality & validation (Great Expectations)

# ============================================================================
# Notes :
# - delta-spark installe automatiquement pyspark >= 3.5.0 si absent
# - faker et confluent-kafka sont facultatifs, utiles en simulation
# - veille à ce que les jars Delta + S3A soient bien présents dans /opt/bitnami/spark/jars
# - les dépendances pour PostgreSQL sont nécessaires pour l’export “to_sql”
# ============================================================================
