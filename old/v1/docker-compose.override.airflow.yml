# ==========================================================================================
# Fichier    : docker-compose.override.airflow.yml (full .env)
# Objectif   : Ajouter Airflow (orchestration, ETL, workflows) √† la stack Sport Data Solution
# Auteur     : Xavier Rousseau | Juin 2025
#
# --- Mode d'emploi ---
# Ce fichier compl√®te (override) le docker-compose principal.
# Il permet :
#   - Un d√©ploiement Airflow d√©sactivable (ajoute/retire juste cet override)
#   - Une configuration 100% pilot√©e par .env (aucune info sensible ici)
#   - Montage direct des dossiers projets, logs, plugins, dbt et great_expectations
#   - S√©quence de d√©marrage orchestr√©e (init ‚Üí webserver ‚Üí scheduler ‚Üí worker ‚Üí flower)
#   - Pr√™t pour la prod, la CI, l‚Äôaudit, la transmission
# ==========================================================================================
services:

  # ========================================================================================
  # 1. sport-airflow-init
  #   üõ† Service d‚Äôinitialisation Airflow :
  #      - Installe les d√©pendances Python (requirements.txt)
  #      - Initialise la base (airflow db init)
  #      - Cr√©e le compte admin (admin user dans l‚ÄôUI)
  #   üîÅ Ne tourne qu‚Äôune fois au d√©marrage (restart: "no")
  #   ‚ö†Ô∏è Doit √™tre relanc√© apr√®s un reset complet de la DB
  # ========================================================================================
  sport-airflow-init:
    build:
      context: .
      dockerfile: Dockerfile.airflow     # Image custom bas√©e sur Airflow officiel
    image: sport-airflow:latest          # Nom de l‚Äôimage personnalis√©e
    container_name: sport-airflow-init
    depends_on:
      - sport-postgres                   # Attend la base de donn√©es
      - sport-redis                      # Attend le broker Celery
    restart: "no"
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW_EXECUTOR}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}/${POSTGRES_DB}
      AIRFLOW__CELERY__BROKER_URL: redis://${REDIS_HOST}:${REDIS_PORT}/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}/${POSTGRES_DB}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/dbt:/opt/airflow/dbt
      - ./airflow/great_expectations:/opt/airflow/great_expectations
      - ./requirements.txt:/requirements.txt
    entrypoint:
      - bash
      - -c
      - |
        pip install -r /requirements.txt && \
        airflow db init && \
        airflow users create \
          --username ${AIRFLOW_ADMIN_USER} \
          --firstname ${AIRFLOW_ADMIN_FIRSTNAME} \
          --lastname ${AIRFLOW_ADMIN_LASTNAME} \
          --role Admin \
          --email ${AIRFLOW_ADMIN_EMAIL} \
          --password ${AIRFLOW_ADMIN_PASSWORD}
    networks:
      - sportdata_net

  # ========================================================================================
  # 2. sport-airflow-webserver
  #   üåê UI web principale (http://localhost:${AIRFLOW_WEB_PORT})
  #      - Visualise, lance, monitore les DAGs
  #      - Peut √™tre red√©marr√© √† chaud (restart: unless-stopped)
  #   ‚ö†Ô∏è  D√©pend de l‚Äôinit pour √™tre s√ªr que la DB/users Airflow existent d√©j√†
  # ========================================================================================
  sport-airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    image: sport-airflow:latest
    container_name: sport-airflow-webserver
    restart: unless-stopped
    depends_on:
      - sport-airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW_EXECUTOR}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}/${POSTGRES_DB}
      AIRFLOW__CELERY__BROKER_URL: redis://${REDIS_HOST}:${REDIS_PORT}/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}/${POSTGRES_DB}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
    ports:
      - "${AIRFLOW_WEB_PORT}:8080"           # Port externe pour acc√®s navigateur
    command: airflow webserver
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/dbt:/opt/airflow/dbt
      - ./airflow/great_expectations:/opt/airflow/great_expectations
      - ./requirements.txt:/requirements.txt
    networks:
      - sportdata_net

  # ========================================================================================
  # 3. sport-airflow-scheduler
  #   ‚è∞ Service planificateur :
  #      - D√©tecte quand d√©clencher/ex√©cuter les DAGs (selon leur schedule/trigger)
  #   ‚ö†Ô∏è  D√©marre apr√®s le webserver (s√©quencement optimal)
  #   ‚ö°Ô∏è  Peut √™tre scal√© si tr√®s grosse volum√©trie (rarement utile hors prod XXL)
  # ========================================================================================
  sport-airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    image: sport-airflow:latest
    container_name: sport-airflow-scheduler
    restart: unless-stopped
    depends_on:
      - sport-airflow-webserver
      - sport-redis
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW_EXECUTOR}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}/${POSTGRES_DB}
      AIRFLOW__CELERY__BROKER_URL: redis://${REDIS_HOST}:${REDIS_PORT}/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}/${POSTGRES_DB}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
    command: airflow scheduler
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/dbt:/opt/airflow/dbt
      - ./airflow/great_expectations:/opt/airflow/great_expectations
      - ./requirements.txt:/requirements.txt
    networks:
      - sportdata_net

  # ========================================================================================
  # 4. sport-airflow-worker
  #   ‚öôÔ∏è  Worker Celery pour ex√©cution distribu√©e des tasks
  #      - Peut √™tre multipli√© si besoin (scaling horizontal)
  #      - Peut √™tre monitor√© par Flower
  #   ‚ö†Ô∏è  D√©marre apr√®s le scheduler pour bonne synchronisation
  # ========================================================================================
  sport-airflow-worker:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    image: sport-airflow:latest
    container_name: sport-airflow-worker
    restart: unless-stopped
    depends_on:
      - sport-airflow-scheduler
      - sport-redis
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW_EXECUTOR}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}/${POSTGRES_DB}
      AIRFLOW__CELERY__BROKER_URL: redis://${REDIS_HOST}:${REDIS_PORT}/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}/${POSTGRES_DB}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
    command: airflow celery worker
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/dbt:/opt/airflow/dbt
      - ./airflow/great_expectations:/opt/airflow/great_expectations
      - ./requirements.txt:/requirements.txt
    networks:
      - sportdata_net

  # ========================================================================================
  # 5. sport-flower
  #   üìà Monitoring des workers Celery via Flower (http://localhost:${AIRFLOW_FLOWER_PORT})
  #      - Visualise l‚Äô√©tat des workers, tasks, files d‚Äôattente, stats d‚Äôex√©cution
  #      - Indispensable pour debug, monitoring ou scaling
  #      - Peut √™tre d√©sactiv√© si usage ultra-minimaliste
  #   ‚ö†Ô∏è  D√©marre apr√®s les workers pour collecter les stats
  # ========================================================================================
  sport-flower:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    image: sport-airflow:latest
    container_name: sport-flower
    restart: unless-stopped
    depends_on:
      - sport-airflow-worker
      - sport-redis
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW_EXECUTOR}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}/${POSTGRES_DB}
      AIRFLOW__CELERY__BROKER_URL: redis://${REDIS_HOST}:${REDIS_PORT}/0
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
    command: airflow celery flower
    ports:
      - "${AIRFLOW_FLOWER_PORT}:5555"
    volumes:
      - ./requirements.txt:/requirements.txt
    networks:
      - sportdata_net


# ==========================================================================================
# FIN DE FICHIER ‚Äî docker-compose.override.airflow.yml
# ==========================================================================================
