# ====================================================================================
# FICHIER   : docker-compose.yml
# OBJET     : Services centraux de la stack Sport Data Solution (ingestion, stockage, traitement)
# AUTEUR    : Xavier Rousseau â€” Juillet 2025
# ====================================================================================

services:

  # ================================================================================
  # 1. PostgreSQL â€” Base de donnÃ©es relationnelle principale
  #    - UtilisÃ© par Airflow, Metabase, pgAdmin, DBT et autres
  #    - Contient les donnÃ©es mÃ©tiers de la plateforme
  # ================================================================================
  sport-postgres:
    image: postgres:15
    container_name: sport-postgres
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${POSTGRES_USER}             # Identifiant de connexion
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}     # Mot de passe PostgreSQL
      POSTGRES_DB: ${POSTGRES_DB}                 # Base de donnÃ©es principale
    ports:
      - "${POSTGRES_PORT}:5432"                   # Port PostgreSQL exposÃ©
    volumes:
      - postgres_data:/var/lib/postgresql/data    # Volume persistant des donnÃ©es
    command:
      - "postgres"
      - "-c"
      - "wal_level=logical"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER}"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - sport-network

  # ================================================================================
  # 2. MinIO â€” Stockage objet S3-compatible
  #    - UtilisÃ© pour stocker les fichiers bruts, logs, exports, rapports
  #    - Console web incluse pour navigation
  # ================================================================================
  sport-minio:
    image: minio/minio:latest
    container_name: sport-minio
    command: server /data --console-address ":${MINIO_CONSOLE_PORT}"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    ports:
      - "${MINIO_PORT}:9000"                       # API S3
      - "${MINIO_CONSOLE_PORT}:9001"               # Console web MinIO
    volumes:
      - minio_data:/data                           # Stockage des objets
    restart: unless-stopped
    networks:
      - sport-network

  # ================================================================================
  # 3. Redpanda â€” Broker Kafka-compatible (streaming temps rÃ©el)
  #    - UtilisÃ© pour gÃ©rer les flux temps rÃ©el (CDC, Ã©vÃ©nements)
  # ================================================================================
  sport-redpanda:
    image: docker.redpanda.com/redpandadata/redpanda:latest
    container_name: sport-redpanda
    command: redpanda start --overprovisioned --smp 1 --memory 512M --reserve-memory 0M --node-id 0 --check=false \
      --advertise-kafka-addr sport-redpanda:9092 --kafka-addr 0.0.0.0:9092
    ports:
      - "${REDPANDA_BROKER_PORT}:9092"
      - "${REDPANDA_ADMIN_PORT}:9644"
    volumes:
      - redpanda_data:/var/lib/redpanda/data
    restart: unless-stopped
    networks:
      - sport-network

  # --------------------------------------------------------------------------------
  # Console Redpanda â€” UI web pour explorer les topics, messages, schÃ©mas
  # --------------------------------------------------------------------------------
  sport-redpanda-console:
    image: docker.redpanda.com/redpandadata/console:latest
    container_name: sport-redpanda-console
    restart: always
    environment:
      - KAFKA_BROKERS=sport-redpanda:9092
      - REDPANDA_ADMIN_URL=http://sport-redpanda:9644
    ports:
      - "${REDPANDA_CONSOLE_PORT}:8080"           # Interface console web
    depends_on:
      - sport-redpanda
    networks:
      - sport-network

  # ================================================================================
  # 4. Debezium â€” Change Data Capture (CDC) PostgreSQL vers Redpanda (Kafka)
  #    - Capture les changements en temps rÃ©el dans PostgreSQL
  #    - Publie les Ã©vÃ©nements dans Redpanda
  # ================================================================================
  sport-debezium:
    image: debezium/connect:2.5
    container_name: sport-debezium
    depends_on:
      sport-postgres:
        condition: service_healthy
      sport-redpanda:
        condition: service_started
    environment:
      BOOTSTRAP_SERVERS: ${DEBEZIUM_BOOTSTRAP_SERVERS}
      GROUP_ID: ${DEBEZIUM_GROUP_ID}
      CONFIG_STORAGE_TOPIC: ${DEBEZIUM_CONFIG_STORAGE_TOPIC}
      OFFSET_STORAGE_TOPIC: ${DEBEZIUM_OFFSET_STORAGE_TOPIC}
      STATUS_STORAGE_TOPIC: ${DEBEZIUM_STATUS_STORAGE_TOPIC}
      REST_ADVERTISED_HOST_NAME: ${DEBEZIUM_CONNECT_REST_ADVERTISED_HOST_NAME}
    ports:
      - "${DEBEZIUM_CONNECT_PORT}:8083"
    restart: unless-stopped
    networks:
      - sport-network
   # ================================================================================
  # 5. Spark â€” Traitement de donnÃ©es (batch, streaming)
  #    - Peut consommer des topics Kafka et Ã©crire en Delta Lake ou PostgreSQL
  #    - ðŸ”” Peut aussi envoyer des notifications via NTFY si utilisÃ© dans les scripts
  # ================================================================================
  sport-spark:
    build:
      context: .
      dockerfile: spark/Dockerfile  # Dockerfile situÃ© dans ./spark/
    image: sport-spark:latest
    container_name: sport-spark

    environment:
      SPARK_MODE: master
      KAFKA_BOOTSTRAP_SERVERS: sport-redpanda:9092

      # ðŸª£ AccÃ¨s MinIO via S3A
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}
      AWS_REGION: us-east-1
      S3_ENDPOINT: http://sport-minio:9000
      S3_PATH_STYLE_ACCESS: true
      PYSPARK_PYTHON: python3

      # ðŸ”” Notifications ntfy
      NTFY_URL: ${NTFY_URL}
      NTFY_TOPIC: ${NTFY_TOPIC}

    ports:
      - "${SPARK_UI_PORT}:4040"
      - "${SPARK_CLUSTER_PORT}:7077"
    volumes:
      - ./spark/jobs:/opt/bitnami/spark/jobs
      - ./spark/conf/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./airflow/scripts:/opt/airflow/scripts:ro
    networks:
      - sport-network
    restart: unless-stopped

  # ================================================================================
  # 6. Redis â€” Broker Celery pour Airflow (exÃ©cuteur distribuÃ©)
  # ================================================================================
  sport-redis:
    image: redis:7
    container_name: sport-redis
    ports:
      - "${REDIS_PORT}:6379"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
    networks:
      - sport-network

  # ================================================================================
  # 7. ntfy â€” Service de notifications simples (via CLI, API REST ou Webhook)
  #    - Notifications dâ€™alertes ou messages techniques
  # ================================================================================
  sport-ntfy:
    image: binwiederhier/ntfy
    container_name: sport-ntfy
    command: serve
    ports:
      - "${NTFY_HTTP_PORT}:80"         # API REST et UI Web (http://localhost:<port>)
    volumes:
      - ./docker/ntfy:/etc/ntfy  # Montre le dossier oÃ¹ est ntfy.yml
    restart: always
    networks:
      - sport-network

# ====================================================================================
# ðŸ”— RÃ©seau Docker partagÃ© entre tous les services
# ====================================================================================
networks:
  sport-network:
    name: sport-network
    driver: bridge

# ====================================================================================
# ðŸ’¾ Volumes persistants (conservation des donnÃ©es et Ã©tats)
# ====================================================================================
volumes:
  postgres_data:
  minio_data:
  redpanda_data:
  spark_data:
